# -*- coding: utf-8 -*-
"""51800574_HuynhTanLoi_Apriori_Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FBRUhtpxMQdec7Q1FJ5rWUn3CV5Byf_l
"""

!pip install pyspark

"""# Thuật toán Apriori trên PySpark

## Import
"""

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
from pyspark.sql import SQLContext
drive.mount('/content/drive',force_remount=True)
import collections
conf = SparkConf().setMaster("local").setAppName("AprioriAlgorithm")
sc = SparkContext.getOrCreate(conf=conf)

"""## Các hàm hổ trợ thuật toán"""

# tạo ra các cặp candidate tiếp theo: C(k) =  tổ hợp chập k của n phần tử L(k-1)
def create_C_k(l_k, k):
  next_c = [var1 | var2 for index, var1 in enumerate(l_k) for var2 in l_k[index + 1:] if list(var1)[:k - 2] == list(var2)[:k - 2]]
  return next_c

# Đưa ra support thoả điều kiện lớn hơn hoặc bằng min_support
def get_support_valid(x, broadcastVar):
  x_sup = len([1 for t in broadcastVar.value if x.issubset(t)])
  if x_sup >= min_sup:
      return x, x_sup
  else:
      return ()

# tạo tập data L_k sau khi lọc các tập data phù hợp min_support
def create_L_k(sc, c_k, broadcastVar, min_sup):
  l_k = sc.parallelize(c_k).map(lambda x : get_support_valid(x, broadcastVar)).filter(lambda x:x).collect()
  return l_k

# Tính confidence
def calculateConfidence(item):
  # Parent item list
  parent = set(item[0][0])
  # Child item list
  # xét điều kiện child item có phải là string
  if(isinstance(item[1][0] , str)):
      child  = set([item[1][0]])
  else:
      child  = set(item[1][0])
  # Parent and Child support values
  parentSupport = item[0][1]
  childSupport = item[1][1]
  # Finds the item set confidence is going to be found
  support = (parentSupport / childSupport)*100
  return list([list(child) ,  list(parent.difference(child)) , support])

def filterForConf(item , total):      
  if(len(item[0][0]) > len(item[1][0])):
    if(checkItemSets(item[0][0] , item[1][0]) == False):
      pass
    else:
      return (item)       
  else:
    pass

# kiểm tra itemlist L_k -> lấy ra được l_k cuối cùng
def checkItemSets(item_1 , item_2):
  if(len(item_1) > len(item_2)):
    return all(any(k == l for k in item_1 ) for l in item_2)
  else:
    return all(any(k == l for k in item_2 ) for l in item_1)

"""## Đọc file store_data.csv

"""

text_file = sc.textFile("drive/MyDrive/BIGDATA/Week3/Apriori/store_data.csv")
# Print in the first of the file
print(text_file.first())

# Split
itemset = text_file.map(lambda line: sorted([item for item in line.strip().split(',')]))
print(itemset.collect())

# Broadcast variables: biến broadcast lưu dữ liệu dưới dạng read-only cached và deserialized trên mỗi cluster -> giảm chi phí "giao tiếp"
broadcastVar = sc.broadcast(itemset.map(lambda x: set(x)).collect())
print(broadcastVar.value)

k = 1
# candidate -> c(1) với k = 1
# flatmap -> flatMap([[1,2], [3,4]]) = [1, 2, 3, 4]
c_k = itemset.flatMap(lambda x: x).distinct().collect()
# chuyển về set() để sử dụng các toán tử
c_k = [{x} for x in c_k]
print("Candidate 1:",len(c_k))
print(c_k)

"""## Chạy thuật toán: min_sup = 150 - min_confidence = 40%"""

min_sup = 150
min_Confidence = 0.4
All_L_itemsets = list()
while len(c_k) > 0:
  print("=====================================================")
  # create l_k
  print("C{}: {}".format(k, c_k))
  l_k = create_L_k(sc, c_k, broadcastVar, min_sup)
  print("L{}: {}".format(k, l_k))

  # x_sup = len([1 for t in broadcastVar.value if {"milk"}.issubset(t)]) # 972

  # filter: lọc các phần tử không phù hợp
  # l_k_test = sc.parallelize(c_k).map(lambda x: len([1 for t in broadcastVar.value if x.issubset(t)])).collect()
  # l_k_test_1 = sc.parallelize(c_k).map(lambda x: len([1 for t in broadcastVar.value if x.issubset(t)])).filter(lambda x : x).collect()
  # test = [1 for t in broadcastVar.value]

  # print(l_k_test)
  # print(l_k_test_1)

  # print([set(item) for item in map(lambda x: x[0], l_k)])

  print("Số lượng L{}: {}".format(k, len(l_k)))
  if len(l_k) != 0:
    for i in l_k:
      # chuyển kiểu dữ liệu -> áp data vào cartesian
      # print((list(x for x in i[0]),i[1]))
      All_L_itemsets.append((list(x for x in i[0]),i[1]))

  # tạo candidate(k+1)
  k += 1
  c_k = create_C_k([set(item) for item in map(lambda x: x[0], l_k)], k)
  
print("=====================================================")
# các cặp có min_sup phù hợp với từng C(k)
rdd = sc.parallelize(All_L_itemsets, numSlices=1)
print(rdd.collect())

"""## Tính confidence và tìm các association rule


"""

# cartesian: Khi gọi 1 tập dữ liệu kiểu T và U, nó sẽ trả về tập dữ liệu mới (T,U)
cacl = rdd.cartesian(rdd)
print(cacl.collect())
print("----------------------")
transition = cacl.count()

# Gán điều kiện filter: true false
baseRddConfidence = cacl.filter(lambda item: filterForConf(item , transition))
# test
print("filter thành parent itemlist và child itemlist (loại bỏ các L trước dùng hàm checkItemSets()):")
print(baseRddConfidence.collect())
print("-------------------------------")
baseRddConfidence = baseRddConfidence.map(lambda item: calculateConfidence(item))
# before -> after : conf
print(baseRddConfidence.collect())

data = baseRddConfidence.collect()
## tạo một dataframe pySpark
sqlContext = SQLContext(sc)
columns = ["Before", "After" , "Confidence (%)"]
confidenceTable = sqlContext.createDataFrame(data = data , schema = columns)

## show dataframe
print("Các association rule của tập L thoả mãn min_support:")
print(confidenceTable.show())

"""## Chọn association rule phù hợp với min_confidence = 40%"""

print('Các Association rule thoả mãn điều kiện (confidence = 40%):')
result = []
# lọc các association rule có conf lớn hơn hoặc bằng min_confidence
for i in data:
  if float(i[-1]) >= float(min_Confidence*100):
    result.append(i)
confidenceValid = sqlContext.createDataFrame(data = result , schema = columns)

print(confidenceValid.show())