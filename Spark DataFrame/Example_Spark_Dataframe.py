# -*- coding: utf-8 -*-
"""Ex2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SLXlpPa2qJqnLuiJ5OxNc7dsvRbnEtbM

# INTSALL PYSPARK
"""

!pip install pyspark

"""# SPARK RDD"""

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
drive.mount('/content/drive')
import collections
conf = SparkConf().setMaster("local").setAppName("count")
sc = SparkContext.getOrCreate(conf=conf)

text_file = sc.textFile("drive/MyDrive/BIGDATA/Week1/exercise2.txt")
# Print in the first of the file
print(text_file.first())

array_text = text_file.first().split(" ")
array_text = [x.replace(",","").replace(".","").lower() for x in array_text if x.isalnum()]
rdd = sc.parallelize(array_text)
# tuple
key = rdd.map(lambda word: (word, 1))
print(key.collect())

print("length of text file:",len(array_text))

# count word
counts = key.reduceByKey(lambda x, y: x + y)
print(counts.collect())

# word most commonly found in the file
word = counts.reduce(lambda x,y: x if x[1] > y[1] else y)
FreqMax = word[1]
# print(FreqMax)
WordMax = [i for i in counts.collect() if i[1] == FreqMax]
print("Word most commonly found in the file:")
print(WordMax)

# test
spark = SparkSession.builder.master("local[2]").appName("WordCount").getOrCreate()
dataList = [("Java", 20000), ("Python", 100000), ("Scala", 3000), ('Java',2300)]
rdd = spark.sparkContext.parallelize(dataList)
counts_test = rdd.reduceByKey(lambda x, y: x + y)
print(counts_test.collect())

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
import collections
spark = SparkSession.builder.master("local[2]").appName("WordCount").getOrCreate()
dataList = ["Java", "Python", "Scala", 'Javascript']
rdd = spark.sparkContext.parallelize(dataList)
key = rdd.map(lambda word: (word, 1))
print(key.collect())

"""
# SPARK DATAFRAME"""

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
import collections
spark = SparkSession.builder.master("local[2]").appName("dataframetest").getOrCreate()
data = [('51800574','Loi','Tan','Huynh','2000-02-26','M',3320),
  ('51800544','Minh','Nhat','Pham','2000-05-19','M',1000),
  ('51800302','Linh','Nhat','Nguyen','2000-09-05','M',4000),
  ('51800112','Nam','Van','Ho','2000-04-01','M',2200),
  ('51800115','Thong','Huy','Luu','2000-07-17','M',3480)
]

columns = ["id","firstname","middlename","lastname","birth","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
# df.show() hiển thị 20 phần tử đầu
df.show()

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import collections
spark = SparkSession.builder.master("local[2]").appName("dataframetest").getOrCreate()
df = spark.read.csv("drive/MyDrive/BIGDATA/Week2/DataFrame/DataAnalyst.csv")
df.printSchema()

"""# SPARK DATAFRAME: EXAMPLE"""

import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import collections
spark = SparkSession.builder.master("local[2]").appName("dataframetest").getOrCreate()
df = spark.read.csv("drive/MyDrive/BIGDATA/Week2/DataFrame/Fifa_players.csv", inferSchema = True, header = True)
df.show()

df.printSchema()

print(df.columns) # Column Names
print(df.count()) # Row Count
len(df.columns)

df.describe('Country').show()
df.describe('Age').show()

df.select('Name','International Caps').show()