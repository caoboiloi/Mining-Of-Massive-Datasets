# -*- coding: utf-8 -*-
"""Exercise_Bank_dataset_51800574_HuynhTanLoi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13btF9wFXcntadvEZ2lps5MncvhiiYc99
"""

!pip install pyspark

import pyspark
import pandas as pd
import numpy as np
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
import time
from pyspark.sql import SQLContext
drive.mount('/content/drive')
import collections
from pyspark.mllib.linalg import *
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover
from pyspark.mllib.linalg import SparseVector
from scipy.spatial import distance
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer
import json
conf = SparkConf().setMaster("local[8]").setAppName("Exercise")
sc = SparkContext.getOrCreate(conf=conf)
spark = SparkSession(sc)
from pyspark.sql.types import StructField
from pyspark.sql.types import StructType, StringType, FloatType
from pyspark.sql.functions import concat, lit
from pyspark.sql.functions import monotonically_increasing_id

df = spark.read.csv('/content/drive/MyDrive/BIGDATA/Week5/bank.csv', header = True, inferSchema=True)
df.printSchema()

df = df.toPandas()
df.deposit = df.deposit.apply(lambda x: 1 if x == 'yes' else 0)

class_name = 'deposit'
df.groupby(class_name).count()

df = spark.createDataFrame(df)

indexers = [StringIndexer(inputCol=column, outputCol=column+"_index").fit(df) for column in list(set(df.columns)-set(['date'])) ]
pipeline = Pipeline(stages=indexers)
df_r = pipeline.fit(df).transform(df)
df_r.show()

df_r = df_r.drop(df_r.deposit_index)

from pyspark.ml.feature import VectorAssembler

eature_names = df_r.columns[17:]
print(feature_names)
assembler = VectorAssembler()
assembler.setInputCols(feature_names).setOutputCol('features')
transformed_data = assembler.transform(df_r)

transformed_data.show()

[training_data, test_data] = transformed_data.randomSplit([0.8,0.2])

training_data.toPandas()

class_name = 'deposit'

from pyspark.ml.classification import LogisticRegression
model = LogisticRegression(featuresCol = 'features',labelCol=class_name, maxIter=30)

M = model.fit(training_data)

predictions = M.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'deposit', metricName = 'accuracy')
print('Logistic Regression Accuracy:', multi_evaluator.evaluate(predictions))