# -*- coding: utf-8 -*-
"""Spark_Car_Dataset_Euclidean_Distances.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFVwHXufmgR_lYus53lY3HAQpSQVKt3k
"""

!pip install pyspark

import pyspark
import pandas as pd
import numpy as np
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from google.colab import drive
import time
from pyspark.sql import SQLContext
drive.mount('/content/drive')
import collections
from pyspark.mllib.linalg import *
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover
from pyspark.mllib.linalg import SparseVector
from scipy.spatial import distance
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer
import json

begin = time.time()
conf = SparkConf().setMaster("local[8]").setAppName("Exercise")
sc = SparkContext.getOrCreate(conf=conf)
from pyspark.sql.types import StructField
from pyspark.sql.types import StructType, StringType, FloatType
from pyspark.sql.functions import concat, lit
from pyspark.sql.functions import monotonically_increasing_id 
sql_sc = SparkSession(sc)
# load data
schema = StructType([\
    StructField("buying", StringType(), True),\
    StructField("maint", StringType(), True),\
    StructField("doors", StringType(), True),\
    StructField("persons", StringType(), True),\
    StructField("lug_boot", StringType(), True),\
    StructField("safety", StringType(), True)])
cars_df = sql_sc.read.format('csv').option("delimiter", ",").schema(schema).option("header", "true").load("/content/drive/MyDrive/BIGDATA/BT/car.data")


cars_df = cars_df.select("*").withColumn("id", monotonically_increasing_id())
cars_df.show()

indexers = [StringIndexer(inputCol=column, outputCol=column+"_index").fit(cars_df) for column in list(set(cars_df.columns)-set(['date'])) ]
pipeline = Pipeline(stages=indexers)
df_r = pipeline.fit(cars_df).transform(cars_df)
df_r = df_r.drop(df_r.id_index)
df_r.show()

from pyspark.ml.feature import VectorAssembler

feature_names = df_r.columns[7:]
print(feature_names)
assembler = VectorAssembler()
assembler.setInputCols(feature_names).setOutputCol('features')
transformed_data = assembler.transform(df_r)

transformed_data.show()

import pyspark.sql.functions as F
from scipy.spatial import distance

# input
input_row = transformed_data.select("features").take(1)[0][0]
# print(input_row)
distance_udf = F.udf(lambda x: float(distance.euclidean(x, input_row)), FloatType())
transformed_data = transformed_data.withColumn('euclidean_distances', distance_udf(F.col('features')))
transformed_data.show()
b = transformed_data.rdd.map(lambda x: x['euclidean_distances'])
result = b.sortBy(lambda x: x,ascending = True).collect()
print("result:")
print(result[:10])